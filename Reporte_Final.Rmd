---
title: "Multivariate Adaptive Regression Splines (MARS)"
author: "Adrian Tame Jacobo, Miguel Calvo Valente, Nelson Gil Vargas"
date: "11/28/2021"
output: pdf_document
---

```{r, warning=FALSE, echo=FALSE, message = FALSE}
# Semilla
set.seed(1286)

# Importamos librerias

library(tidyverse)
library(tidymodels)
library(earth)
library(caret)
library(latex2exp)
library(patchwork)
```

## Introducción

El modelo MARS es un algoritmo de aprendizaje supervisado que funciona para regresión. En términos simples, es una versión modificada de regresión lineal, pero el modelo es más complicado ya que permite hacer diferentes pendientes para diferentes partes de la variable a estimar, y automaticamente modela términos no lineales e interacciones entre variables. Fue introducido originalmente en [1] por Friedman en 1991, y existen varias implementaciones del algoritmo, generalmente bajo el nombre de "Earth" [CITA] [CITA].  

## Comportamiento del Modelo

En términos simples, podemos considerar a MARS como un modelo de regresión lineal a pedazos. Como se aprecia en el ejemplo de abajo, MARS busca los puntos de corte y las pendientes óptimas para aproximar a la variable objetivo:

```{r, fig.width = 10, fig.height = 5, echo=FALSE}
x <- seq(-5, 10, .1)
y <- -0.05*x^4 + 0.5*x^3 + 0.5*x^2 - 10*x 
z <- rnorm(length(x), sd = 10)

ejemplo_1 <- tibble(x = x, y = y, sims = y + z)

ejemplo_1 |>
  ggplot() +
  geom_line(aes(x = x, y = y), color = "black", size = 1, alpha = 0.8) +
  geom_point(aes(x = x, y = sims), alpha = 0.2, size = 3, shape = 16, stroke = 0) +
  ggtitle(TeX(r'($y = -\frac{1}{20}x^4 +\frac{1}{2}x^3 + \frac{1}{2}x^2 - 10x + \epsilon$)')) + 
  theme_classic()
```

```{r, fig.width = 10, fig.height = 5, echo=FALSE}
n <- 4

mars_1.1 <- earth(
  sims ~ x,  
  data = ejemplo_1,
  nprune = n+1,
  degree = n,
  )

preds <- predict(mars_1.1, ejemplo_1)

ejemplo_1 |>
  mutate(preds = preds) |>
  ggplot() +
  # geom_vline(xintercept = c(-2.3, 3.7, 6.9, 8.5), color = "black", size = 1, alpha = 0.5) +
  geom_line(aes(x = x, y = y), color = "black", size = 1, alpha = 0.5) +
  geom_point(aes(x = x, y = sims), alpha = 0.2, size = 3, shape = 16, stroke = 0) +
  geom_line(aes(x = x, y = preds), color = "darkblue", size = 1.5) + 
  ggtitle(paste("MARS con",n,"nodos")) + 
  theme_classic()
```

Como podemos ver, de darnos a la tarea de encontrar manualmente cada punto de corte y pendientes se puede traducir a minimizar alguna métrica como el error cuadrático medio:

```{r, fig.width = 10, fig.height = 5, warning=FALSE, echo=FALSE, message = FALSE}
x <- seq(-10, 10, .1)
y <- -0.1*x^3+ 10*x
z <- rnorm(length(x), sd = 20)

ejemplo_2 <- tibble(x = x, y = y, sims = y + z)
```

```{r, fig.width = 5, fig.height = 5,warning=FALSE, echo=FALSE, message = FALSE}
cutoff <- -5
linea_izquierda <- (-110 - 10*x)*ifelse(x <= cutoff, 1, NA)
linea_derecha <- (-25 + 7*x)*ifelse(x >= cutoff, 1, NA)

ejemplo_2 <- tibble(x = x,
                    y = y,
                    sims = y + z,
                    linea_derecha = linea_derecha, 
                    linea_izquierda = linea_izquierda, 
                    residuales_derecha = sims - linea_derecha,
                    residuales_izquierda = sims - linea_izquierda,
                    residuales_derecha_top = ifelse(residuales_derecha > 0, residuales_derecha, 0),
                    residuales_izquierda_top = ifelse(residuales_izquierda > 0, residuales_izquierda, 0),
                    residuales_derecha_bot = ifelse(residuales_derecha < 0, residuales_derecha, 0),
                    residuales_izquierda_bot = ifelse(residuales_izquierda < 0, residuales_izquierda, 0),
                    )

g1 <- ejemplo_2 |>
  ggplot() +
  geom_point(aes(x = x, y = sims), alpha = 0.2, size = 2, shape = 16, stroke = 0) +
  geom_vline(xintercept = cutoff, size = 1, color = "darkblue") +
  geom_line(aes(x = x, y = linea_izquierda), color = "blue", size = 1, alpha = 0.8) +
  geom_line(aes(x = x, y = linea_derecha), color = "blue", size = 1, alpha = 0.8) +
  geom_linerange(aes(x = x, ymax = residuales_derecha_top,  ymin = residuales_derecha_bot),
                alpha = 0.3, size = 1, shape = 16, stroke = 0, color = "red") +
  geom_linerange(aes(x = x, ymax = residuales_izquierda_top,  ymin = residuales_izquierda_bot),
                alpha = 0.3, size = 1, shape = 16, stroke = 0, color = "red") +
  ylab("y") + xlab("x") +
  theme_classic()
```

```{r, fig.width = 5, fig.height = 5, warning=FALSE, echo=FALSE, message = FALSE}
cutoff <- 0
linea_izquierda <- (-30 - 1*x)*ifelse(x <= cutoff, 1, NA)
linea_derecha <- (-30 + 10*x)*ifelse(x >= cutoff, 1, NA)

ejemplo_2 <- tibble(x = x,
                    y = y,
                    sims = y + z,
                    linea_derecha = linea_derecha, 
                    linea_izquierda = linea_izquierda, 
                    residuales_derecha = sims - linea_derecha,
                    residuales_izquierda = sims - linea_izquierda,
                    residuales_derecha_top = ifelse(residuales_derecha > 0, residuales_derecha, 0),
                    residuales_izquierda_top = ifelse(residuales_izquierda > 0, residuales_izquierda, 0),
                    residuales_derecha_bot = ifelse(residuales_derecha < 0, residuales_derecha, 0),
                    residuales_izquierda_bot = ifelse(residuales_izquierda < 0, residuales_izquierda, 0),
                    )

g2 <- ejemplo_2 |>
  ggplot() +
  geom_point(aes(x = x, y = sims), alpha = 0.2, size = 2, shape = 16, stroke = 0) +
  geom_vline(xintercept = cutoff, size = 1, color = "darkblue") +
  geom_line(aes(x = x, y = linea_izquierda), color = "blue", size = 1, alpha = 0.8) +
  geom_line(aes(x = x, y = linea_derecha), color = "blue", size = 1, alpha = 0.8) +
  geom_linerange(aes(x = x, ymax = residuales_derecha_top,  ymin = residuales_derecha_bot),
                alpha = 0.3, size = 1, shape = 16, stroke = 0, color = "red") +
  geom_linerange(aes(x = x, ymax = residuales_izquierda_top,  ymin = residuales_izquierda_bot),
                alpha = 0.3, size = 1, shape = 16, stroke = 0, color = "red") +
  ylab("y") + xlab("x") +
  theme_classic()
```

```{r, fig.width = 5, fig.height = 5, warning=FALSE, echo=FALSE, message = FALSE}
cutoff <- 5
linea_izquierda <- (10 + 8*x)*ifelse(x <= cutoff, 1, NA)
linea_derecha <- (110 - 12*x)*ifelse(x >= cutoff, 1, NA)

ejemplo_2 <- tibble(x = x,
                    y = y,
                    sims = y + z,
                    linea_derecha = linea_derecha, 
                    linea_izquierda = linea_izquierda, 
                    residuales_derecha = sims - linea_derecha,
                    residuales_izquierda = sims - linea_izquierda,
                    residuales_derecha_top = ifelse(residuales_derecha > 0, residuales_derecha, 0),
                    residuales_izquierda_top = ifelse(residuales_izquierda > 0, residuales_izquierda, 0),
                    residuales_derecha_bot = ifelse(residuales_derecha < 0, residuales_derecha, 0),
                    residuales_izquierda_bot = ifelse(residuales_izquierda < 0, residuales_izquierda, 0),
                    )

g3 <- ejemplo_2 |>
  ggplot() +
  geom_point(aes(x = x, y = sims), alpha = 0.2, size = 2, shape = 16, stroke = 0) +
  geom_vline(xintercept = cutoff, size = 1, color = "darkblue") +
  geom_line(aes(x = x, y = linea_izquierda), color = "blue", size = 1, alpha = 0.8) +
  geom_line(aes(x = x, y = linea_derecha), color = "blue", size = 1, alpha = 0.8) +
  geom_linerange(aes(x = x, ymax = residuales_derecha_top,  ymin = residuales_derecha_bot),
                alpha = 0.3, size = 1, shape = 16, stroke = 0, color = "red") +
  geom_linerange(aes(x = x, ymax = residuales_izquierda_top,  ymin = residuales_izquierda_bot),
                alpha = 0.3, size = 1, shape = 16, stroke = 0, color = "red") +
  ylab("y") + xlab("x") +
  theme_classic()
```

```{r, fig.width = 12, fig.height = 5, warning=FALSE, echo=FALSE, message = FALSE}
g1 + g2 + g3
```

Dados los valores anteriores, vemos el ajuste realizado por MARS:

```{r, fig.width = 5, fig.height = 5, warning=FALSE, echo=FALSE, message = FALSE}
n <- 1

mars_2.1 <- earth(
  sims ~ x,  
  data = ejemplo_2,
  nprune = n+1,
  degree = n,
  )

preds <- predict(mars_2.1, ejemplo_2)

ejemplo_2.1 <- tibble(x = x,
                    y = y,
                    sims = y + z,
                    linea_derecha = linea_derecha, 
                    linea_izquierda = linea_izquierda,
                    preds = preds,
                    residuales = sims - preds,
                    residuales_top = ifelse(residuales > 0, residuales, 0),
                    residuales_bot = ifelse(residuales < 0, residuales, 0),
                    )

ejemplo_2.1 |>
  mutate(preds = preds) |>
  ggplot() +
  # geom_vline(xintercept = c(-2.3, 3.7, 6.9, 8.5), color = "black", size = 1, alpha = 0.5) +
  # geom_line(aes(x = x, y = y), color = "black", size = 1, alpha = 0.5) +
  geom_point(aes(x = x, y = sims), alpha = 0.2, size = 2, shape = 16, stroke = 0) +
  geom_line(aes(x = x, y = preds), color = "darkblue", size = 1.5) + 
  geom_linerange(aes(x = x, ymax = residuales_top,  ymin = residuales_bot),
                alpha = 0.3, size = 1, shape = 16, stroke = 0, color = "red") +
  ggtitle(paste("MARS con",n,"nodo")) + 
  ylab("y") + xlab("")+
  theme_classic()
```

## Descripción del Modelo

MARS es un modelo de regresión no paramétrico que a través de funciones bisagra (*hinge functions*). Se puede considerar como una modificación al método CART con el fin de mejorar su desempeño en el contexto de regresión Es un modelo que es bueno para problemas de alta dimensionalidad. Para la siguiente sección, usamos la terminología y seguimos el desarrollo que se presenta en [1] y en [2]. 

MARS se define con el uso de funciones que son lineales por partes, de la forma $(x - t)_+$ y $(t - x)_+$. Estas funciones toman el máximo entre 0 y el valor dentro de la funcion, por lo tanto, 
$$
(x - t)_+ = \max\{ 0, x - t \}= \begin{cases} 
x - t &\mbox{si } x > t, \\
0 & \mbox{en otro caso}. \end{cases}
$$
Cada función es lineal a trozos con un cambio de pendiente en el valor $t$, lo cual las hace splines lineales, y a cada par dividido en el valor $t$ se le llama un *par reflejado*. 

La idea del método es formar pares reflejados para cada variable de entrada $X_j$ con cambios de pendiente en el spline en cada valor observado $x_{ij}$. Por lo tanto, para cada variable de entrada o variable predictiva, se define un spline distinto. Por lo tanto, para una variable $X_j$, la colección de funciones base es 
$$
\mathcal{C} = \{ (X_j - t)_+, (t - X_j)_+ \}, \text{ con } t \in \{ x_{1,j}, x_{2,j}, \ldots x_{N,j} \}, \text{ y } \,\, j = 1,2, \ldots , p.
$$
Esto significa que si todos los valores de entrada son distintos, se tienen $2Np$ funciones base en total, y $2N$ divisiones en cada uno de los splines para cada variable. El modelo que utilizamos entonces para juntar todas las variables es uno aditivo, de forma parecida que se hace con regresión lineal: 
$$
f(X) = \beta_0 + \sum_{m=1}^M \beta_m h_m(X),
$$
donde cada función $h_m(X)$ es elemento de $\mathcal{C}$ o una combinación lineal de estas funciones base, pero para esta explicación inicial, usaremos solamente funciones que no involucran interacciones. Este modelo sin interacciones funciona de forma muy parecida a los árboles de decisión CART. Visualmente, se ve algo parecido a la siguiente gráfica cada uno de los pares reflejados que se usan (notamos que estas están en $x=0$ por facilidad de visualización): 

```{r, fig.width = 8, fig.height = 2.5, warning=FALSE, echo=FALSE, message = FALSE}
x_i <- c(-3, 0, 3)
x_1 <- seq(-5, -1, 1)
y_1 <- ifelse(x_1 > x_i[1], x_1 - x_i[1], x_i[1] - x_1)
x_2 <- seq(-2, 2, 1)
y_2 <- ifelse(x_2 > x_i[2], x_2 - x_i[2], x_i[2] - x_2)
x_3 <- seq(1, 5, 1)
y_3 <- ifelse(x_3 > x_i[3], x_3 - x_i[3], x_i[3] - x_3)
ejemplo_4 <- tibble(x_1 = x_1, y_1 = y_1,
                    x_2 = x_2, y_2 = y_2,
                    x_3 = x_3, y_3 = y_3)

ejemplo_4 |>
  ggplot() +
  geom_line(aes(x = x_1, y = y_1), color = "blue", size = 1.5, alpha = 0.7) + 
  geom_line(aes(x = x_2, y = y_2), color = "darkblue", size = 1.5, alpha = 0.7) + 
  geom_line(aes(x = x_3, y = y_3), color = "black", size = 1.5, alpha = 0.7) + 
  # ggtitle(paste("MARS con",n,"nodo")) + 
  ylab("y") + xlab("x")+
  theme_classic()
```

El proceso de construcción del modelo (denotado $\mathcal{M}$) empieza con el modelo base $\hat f(X) = \hat \beta_0 = h_0(X) = 1$, donde se define un término que funciona como intercepto al orígen, y se hace un procedimiento que se llama *forward*. Después de esto, todas las funciones en el conjunto $\mathcal{C}$ son candidatas de entrada a $\mathcal{M}$. Para cada observación $x_{ij}$, se genera un punto de corte descrito por un par reflejado: 
$$
h_1(X) = h(X_j - x_{ij}) 
$$
$$
h_2(X) = h(x_{i,j} - X_j)
$$

en la siguiente gráfica, podemos ver una representación de como se verían estas funciones: 

```{r, fig.width = 5, fig.height = 3, warning=FALSE, echo=FALSE, message = FALSE}
x_left <- seq(-5, -3, 1)
x_right <- seq(-3, -1, 1)
y_left <- -(x_left + 3)
y_right <- 3 + x_right

ejemplo_4 <- tibble(x_left = x_left, x_right = x_right,
                    y_left = y_left, y_right = y_right)

ejemplo_4 |>
  ggplot() +
  geom_line(aes(x = x_left, y = y_left), color = "cyan4", size = 1.5, alpha = 0.7) +
  geom_line(aes(x = x_right, y = c(0,0,0)), color = "cyan4", size = 1.5, alpha = 0.7, linetype = "dashed") +
  geom_line(aes(x = x_right, y = y_right), color = "navyblue", size = 1.5, alpha = 0.7) + 
  geom_line(aes(x = x_left, y = c(0,0,0)), color = "navyblue", size = 1.5, alpha = 0.7, linetype = "dashed") +
  # ggtitle(paste("MARS con",n,"nodo")) + 
  annotate("text", x = -1.9, y = 1.9, label = TeX(r'($h_{1} = (X_j-x_{ij})_+$)'), size = 5.5) +
  annotate("text", x = -4.1, y = 1.9, label = TeX(r'($h_{2} = (x_{ij}-X_j)_+$)'), size = 5.5) +
  ylab("y") + xlab("x")+
  theme_classic()
```

y con esto, se ajusta el nuevo modelo involucrando estas funciones:
$$
\hat f(X) = \hat \beta_0 + \hat \beta_1 h_1(X) + \hat \beta_2 h_2(X).
$$
Como esto es una forma lineal en términos de cada una de las funciones $h_i$, se hace el ajuste de cada parámetro $\hat \beta_i$ minimizando la suma de cuadrados. 

En cada etapa de este procedimiento, se van agregando pares reflejados para cada una de las observaciones que se tienen, y eventualmente, se llega a un modelo final que incluye todos los posibles cortes definidos por cada observación. Claramente, esto llevará a sobreajuste de los datos, pero esto es lo que se está buscando en esta parte del procedimiento. Ya teniendo $\mathcal{M}$ con todas las interacciones posibles, se empieza la segunda parte del ajuste del modelo, que es la parte de podarlo. En este caso, se van eliminando los términos $h_i(X)$ iterativamente, empezando por el que produce el menor incremento en el error cuadrático residual cuando se quita. Este procedimiento produce un mejor modelo para cada tamaño $\lambda$, donde este modelo lo denotamos $\hat f_\lambda$. 

Hay varios procedimientos que se pueden utilizar para estimar el valor óptimo de $\lambda$, como validación cruzada o bootstrap, y probablemente el mejor de ellos sea análisis de *leave one out*, pero esto involucra un gran costo computacional. Para minimizar este problema de costo computacional, los modelos MARS generalmente utilizan un procedimiento de validación cruzada generalizada (GCV, por sus siglas en inglés). Este criterio se define como: 
$$
GCV(\lambda) = \frac{ \sum_{i=1}^N (y_i - \hat f_\lambda(x_i))^2 }{\left( \frac{1 - M(\lambda)}{N} \right)^2},
$$
donde el valor $M(\lambda)$ es el número de parámetros *efectivos* en el modelo, que depende del número de términos más el número de puntos de corte utilizados penalizado por un factor (2 en el caso aditivo que estamos explicando, 3 cuando hay interacciones). 

Ya que describimos el modelo base, podemos regresar y considerar cuales serían las diferencias al incorporar términos de interacción. Si se tiene el modelo base sin podar, podemos seguir iterando y agregando términos, pero esta vez, se considera como nueva función base todos los productos de una cierta función $h_m$ en el modelo $\mathcal{M}$ con uno de los pares reflejados que definimos en $\mathcal{C}$. Estos se agregan al modelo de la siguiente forma: 
$$
\hat f(X) = \hat \beta_0 + \sum_{m=1}^M \hat \beta_m h_m(X) + \hat \beta_{M+1} h_\ell(X) \cdot (X_j - t)_+ + \hat \beta_{M+2} h_\ell(X) \cdot (t-X_j)_+,
$$
para cada $h_\ell \in \mathcal{M}$, y donde $t = x_{ij}$ cualquiera. Otra manera de ver esto es que el modelo base es interactuar cada una de los pares reflejados para una variable $x_{ij}$ con el término incial del modelo, $h_0 = \hat \beta_0 = 1$. 

## Implementación

Para nuestra implementación, utilizamos una base de datos `spam`, tomada de [3]. Es una colección de palabras y caracteres que aparecen comúnmente en mensajes que son *spam*. La variable respuesta es una variable categórica que tiene dos niveles: `spam`, `no_spam`. Más información se puede ver en [4]. 

Presentamos una table de algunas de las variables predictoras y la variable respuesta.

```{r, warning=FALSE, echo=FALSE, message = FALSE}
# Cargamos datos de entrenamiento
spam_entrena <- read_csv('./datos/spam-entrena.csv') |> 
  mutate(spam = ifelse(spam == 0, "no_spam", "spam")) |> 
  mutate(spam = factor(spam))
# Cargamos datos de prueba
spam_prueba <- read_csv('./datos/spam-prueba.csv') |> 
  mutate(spam = ifelse(spam == 0, "no_spam", "spam")) |> 
  mutate(spam = factor(spam)) 
head(spam_entrena |> select(wfyour,wfaddress,wfemail,crlaverage,spam))
```

```{r, warning=FALSE, echo=FALSE, message = FALSE}
# modelo
modelo_mars <- mars(mode = "classification", prod_degree = 5, prune_method = "backward") |>
  set_engine("earth")
# receta
spam_receta <- recipe(spam ~ ., spam_entrena) |> 
  step_relevel(spam, ref_level = "no_spam", skip = TRUE)
# flujo
spam_flujo_1 <- workflow() |> 
  add_recipe(spam_receta) |> 
  add_model(modelo_mars) 
modelo_mars <- fit(spam_flujo_1, spam_entrena)
```

Podemos hacer predicciones con este modelo. Por ejemplo, en entrenamiento tenemos las predicciones de clase dan:

```{r, warning=FALSE, echo=FALSE}
# Definimos las métricas de desempeño
metricas_spam <- metric_set(roc_auc, accuracy, sens, spec)

preds_entrena <- predict(modelo_mars, spam_entrena, type = "prob") |> 
  bind_cols(predict(modelo_mars, spam_entrena)) |> 
  bind_cols(spam_entrena |> select(spam))
preds_entrena |> 
  metricas_spam(spam, .pred_no_spam, estimate = .pred_class) |> 
  mutate(across(is_double, round, 2))
```

y en prueba: 

```{r, warning=FALSE, echo=FALSE}
preds_prueba <- predict(modelo_mars, spam_prueba, type = "prob") |> 
  bind_cols(predict(modelo_mars, spam_prueba)) |> 
  bind_cols(spam_prueba |> select(spam))
res_mars <- preds_prueba |> 
  metricas_spam(spam, .pred_no_spam, estimate = .pred_class) |> 
  mutate(across(is_double, round, 2)) 
res_mars
```

Y notamos el ajuste entre prueba y entrenamiento, es bastante consistente por lo que tenemos un buen ajuste, bajo la métrica de `binary`, que es pérdida logarítmica. En la siguiente tabla, podemos ver los coeficientes que se usan para este modelo final (los primeros 10). Notamos que muchos de ellos son interacciones de variables, como se explicó en la descripción del modelo. El valor bajo `spam` es el coeficiente asociado $\beta_i$. 

```{r, warning=FALSE, echo=FALSE}
mars2 <- earth(
  spam ~ .,  
  data = spam_entrena,
  degree = 2)

summary(mars2) %>% .$coefficients %>% head(10)
```

## Comparación con Árboles y Bosques Aleatorios

Ajustamos otros dos modelos, en este caso un árbol de decisiones y bosques aleatorios, y podemos comparar el ajuste de MARS contra este nuevo modelo. Estas implementacióoes para estos datos la tomamos de [5]. 

```{r,warning=FALSE, echo=FALSE, message = FALSE}
spam_arbol <- decision_tree(cost_complexity = 0, 
                            min_n = 1) |> 
  set_engine("rpart") |> 
  set_mode("classification") |> 
  set_args(model = TRUE)
# receta
spam_receta <- recipe(spam ~ ., spam_entrena) |> 
  step_relevel(spam, ref_level = "no_spam", skip = TRUE)
# flujo
spam_flujo_1 <- workflow() |> 
  add_recipe(spam_receta) |> 
  add_model(spam_arbol) 
arbol_grande <- fit(spam_flujo_1, spam_entrena)
```

```{r,warning=FALSE, echo=FALSE, message = FALSE}
# Desempeño en prueba

preds_prueba <- predict(arbol_grande, spam_prueba, type = "prob") |> 
  bind_cols(predict(arbol_grande, spam_prueba)) |> 
  bind_cols(spam_prueba |> select(spam))
res_arboles <- preds_prueba |> 
  metricas_spam(spam, .pred_no_spam, estimate = .pred_class) |> 
  mutate(across(is_double, round, 2)) 
```

```{r,warning=FALSE, echo=FALSE, message = FALSE}
spam_bosque <- rand_forest(mtry = 6, trees = 1000) |> 
  set_engine("ranger", importance = "permutation") |> 
  set_mode("classification")
# flujo
spam_flujo_2 <- workflow() |> 
  add_recipe(spam_receta) |> 
  add_model(spam_bosque) 
flujo_ajustado <- fit(spam_flujo_2, spam_entrena)
```

```{r,warning=FALSE, echo=FALSE, message = FALSE}
bosque <- extract_fit_parsnip(flujo_ajustado)
```

```{r,warning=FALSE, echo=FALSE, message = FALSE}
res_bosque <- predict(flujo_ajustado , spam_prueba, type = "prob") |>
  bind_cols(predict(flujo_ajustado, spam_prueba)) |> 
  bind_cols(spam_prueba |> select(spam)) |> 
  metricas_spam(spam, .pred_no_spam, estimate = .pred_class) |> 
  mutate(across(is_double, round, 2))
```

```{r,warning=FALSE, echo=FALSE}
res_mars |> 
  mutate(estimate_mars = .estimate, estimate_arboles = res_arboles$.estimate, 
         estimate_bosques = res_bosque$.estimate ) |>
  select(-.estimate)
```

### Curvas Precision-Recall y ROC

Podemos visualizar esta diferencia con una gráfica de precision-recall: 

```{r,warning=FALSE, echo=FALSE}
modelos <- list(arbol_grande = arbol_grande, mars = modelo_mars,  bosque = bosque)
prec_tbl <- map(names(modelos), function(mod_nombre){
  predict(modelos[[mod_nombre]], spam_prueba, type = "prob") |> 
    bind_cols(spam_prueba |> select(spam)) |> 
    pr_curve(spam, .pred_no_spam) |> 
    mutate(modelo = mod_nombre)
  }) |> 
  bind_rows()
ggplot(prec_tbl, 
       aes(x = recall, y = precision, colour = modelo)) + 
 geom_path() + geom_point(size = 1) 
```

O las curvas ROC:

```{r,warning=FALSE, echo=FALSE}
roc_tbl <- map(names(modelos), function(mod_nombre){
  predict(modelos[[mod_nombre]], spam_prueba, type = "prob") |> 
    bind_cols(spam_prueba |> select(spam)) |> 
    roc_curve(spam, .pred_no_spam) |> 
    mutate(modelo = mod_nombre)
  }) |> 
  bind_rows()
ggplot(roc_tbl, 
       aes(x = 1 - specificity, y = sensitivity, colour = modelo)) + 
  geom_point(size= 0.5) + geom_path()
```

En el caso de comparar con otros modelos, vemos desempeño comparable o un poco peor del modelo MARS contra el de bosques aleatorios. 

## Ventajas y Desventajas



## Conclusiones

Spam malo porque clasificación. 

## Referencias

* [1] Friedman, J. H. (1991). "Multivariate Adaptive Regression Splines". *The Annals of Statistics*. **19** (1): 1–67.





https://projecteuclid.org/journals/annals-of-statistics/volume-19/issue-1/Multivariate-Adaptive-Regression-Splines/10.1214/aos/1176347963.full

https://towardsdatascience.com/mars-multivariate-adaptive-regression-splines-how-to-improve-on-linear-regression-e1e7a63c5eae

https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline

[2] https://rubenfcasal.github.io/aprendizaje_estadistico/mars.html

https://web.stanford.edu/~hastie/Papers/ESLII.pdf

[3] https://archive.ics.uci.edu/ml/datasets/spambase

[4] https://lorrie.cranor.org/pubs/spam/

[5] https://aprendizaje-maquina-2021-mcd.netlify.app/m%C3%A9todos-basados-en-%C3%A1rboles.html